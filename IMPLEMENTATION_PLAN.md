# Heretic Implementation Plan
**Based on Codebase Analysis - 2026-01-28**
**Last Updated: 2026-01-29 (Post-optimization review)**

## Executive Summary

This plan addresses the key recommendations from the comprehensive codebase analysis. The project has a quality score of 8.2/10, with the primary gap being automated testing (2/10). Implementation is organized into three phases over 8-12 weeks.

**Critical Priorities:**
1. Add pytest test suite (eliminates primary risk)
2. Fix API key validation security vulnerability
3. Document web search behavior for users

**âœ… Performance Optimizations COMPLETED (2026-01-29):**
- In-memory weight caching: ~5-10x faster model reset (`model.py:87-88`)
- torch.compile() support: ~1.5-2x inference speedup (`config.py:70-73`, `model.py:90-92`)
- Early stopping for refusals: ~40-60% faster refusal counting (`config.py:85-88`, `evaluator.py:62-66`)
- Parallel KL + refusal evaluation: Concurrent execution via ThreadPoolExecutor (`evaluator.py:77-84`)
- Optuna persistence with storage/study_name for resume support (`config.py:108-117`)

**Remaining Performance Opportunities:**
- Multi-GPU parallel execution: 2-4x speedup potential (complex, 60-80h)
- Optuna pruning optimization: ~30% compute savings (requires single-objective mode)

---

## Task Dependency Graph

```
Phase 1 (Critical)
â”œâ”€â”€ Task 1.1 (Testing) â”€â”€â”€â”€â”€â”¬â”€â”€â–º Task 1.2 (Security - needs tests)
â”‚                           â””â”€â”€â–º All Phase 2 tasks (safety net)
â”œâ”€â”€ Task 1.3 (Docs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Independent
â””â”€â”€ Task 1.4 (OOM Recovery) â”€â”€â”€â”€â–º Task 2.3 (Multi-GPU needs OOM handling)

Phase 2 (Performance + Quality)
â”œâ”€â”€ Task 2.1 (Benchmark Optuna) â–º Independent (completed optimizations change baseline)
â”œâ”€â”€ Task 2.2 (Checksums) â”€â”€â”€â”€â”€â”€â”€â–º Independent
â”œâ”€â”€ Task 2.3 (Multi-GPU) â”€â”€â”€â”€â”€â”€â”€â–º Requires PostgreSQL (external dependency)
â”œâ”€â”€ Task 2.4 (Checkpoint) â”€â”€â”€â”€â”€â”€â–º Pairs with Task 2.3
â””â”€â”€ Task 2.5 (CI/CD) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Requires Task 1.1 (tests must exist)

Phase 3 (Architecture)
â”œâ”€â”€ Task 3.1 (Plugins) â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Independent
â””â”€â”€ Task 3.2 (Logging) â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Independent
```

---

## Definition of Done (All Tasks)

Every task must meet these criteria before being marked complete:

- [ ] Code merged to main branch
- [ ] Tests pass (>70% coverage maintained)
- [ ] Ruff linting passes (`uv run ruff check src/heretic/`)
- [ ] Documentation updated (CLAUDE.md, README.md as appropriate)
- [ ] CHANGELOG.md entry added (if user-facing change)
- [ ] Peer review completed (or self-review for small changes)
- [ ] Rollback procedure verified

---

## Phase 1: Critical Fixes (Week 1-2)

### Task 1.1: Add Pytest Test Suite
**Priority:** ðŸ”´ CRITICAL
**Effort:** 32-40 hours (revised: mocking LLMs is complex)
**Risk:** HIGH (No tests = regression risk on all changes)
**Dependencies:** None

**Testing Strategy:**
- **Unit tests:** Use proper mocking with `pytest-mock` and `unittest.mock.patch`
- **Integration tests:** Use tiny models (`gpt2`) for real inference tests, marked `@pytest.mark.slow`
- **CI:** Run unit tests only (fast feedback), nightly runs integration tests
- **Fixtures:** Place in `tests/conftest.py` (NOT `tests/fixtures/conftest.py`)

**Implementation:**
```bash
# 1. Add pytest to dependencies
# Edit pyproject.toml:
[dependency-groups]
dev = [
    "ruff>=0.14.5",
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
]

# 2. Create test structure (NOTE: conftest.py at tests/ root, not in subdirectory)
mkdir -p tests/{unit,integration}
touch tests/__init__.py
touch tests/conftest.py  # <-- CORRECT location for fixtures
touch tests/unit/{test_evaluator.py,test_model.py,test_config.py,test_vast.py}
touch tests/integration/test_abliteration_flow.py

# 3. Configure pytest in pyproject.toml (NOT pytest.ini)
cat >> pyproject.toml << EOF

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "--cov=src/heretic --cov-report=term-missing"
markers = [
    "slow: marks tests as slow (deselect with '-m not slow')",
    "integration: marks integration tests",
    "gpu: marks tests requiring GPU",
]

[tool.coverage.run]
omit = ["tests/*"]

[tool.coverage.report]
fail_under = 70
EOF

# 4. Run tests (unit only, fast)
uv sync --dev
uv run pytest -m "not slow"

# 5. Run all tests including integration
uv run pytest
```

**Test Files to Create:**

**tests/conftest.py:** (CORRECTED - proper mocking approach)
```python
"""Pytest fixtures for heretic tests.

IMPORTANT: This file must be at tests/conftest.py for auto-discovery.
DO NOT place in tests/fixtures/ subdirectory.
"""
import pytest
from unittest.mock import Mock, MagicMock, patch
import torch


@pytest.fixture
def mock_settings():
    """Mock Settings object with common defaults.
    
    Note: Must patch load_prompts before creating Evaluator,
    as Evaluator.__init__ calls load_prompts immediately.
    """
    from heretic.config import Settings
    
    # Create real Settings object with minimal config
    # This avoids issues with Mock not having proper Pydantic behavior
    settings = Settings(
        model="mock-model",
        batch_size=4,
        refusal_markers=["sorry", "i can't", "i cannot", "harmful"],
        system_prompt="You are a helpful assistant.",
        # Use empty/minimal prompts to avoid file loading
        good_prompts="",
        bad_prompts="",
        good_evaluation_prompts="",
        bad_evaluation_prompts="",
    )
    return settings


@pytest.fixture
def mock_tokenizer():
    """Mock tokenizer for testing."""
    tokenizer = MagicMock()
    tokenizer.apply_chat_template.return_value = "<|user|>Hello<|assistant|>"
    tokenizer.encode.return_value = [1, 2, 3, 4, 5]
    tokenizer.decode.return_value = "Hello, I can help with that!"
    tokenizer.pad_token_id = 0
    tokenizer.eos_token_id = 2
    return tokenizer


@pytest.fixture
def mock_model(mock_tokenizer):
    """Mock Model object for testing.
    
    This mocks the heretic Model class, not the HuggingFace model.
    """
    model = MagicMock()
    model.tokenizer = mock_tokenizer
    model.device = torch.device("cpu")
    
    # Mock layer structure (32 layers typical)
    mock_layers = [MagicMock() for _ in range(32)]
    model.get_layers.return_value = mock_layers
    
    # Mock inference methods
    model.get_responses_batched.return_value = [
        "Sure, I can help with that!",
        "Here's how to do it...",
        "I'm sorry, I can't help with that.",  # Refusal
    ]
    
    # Mock logprobs (batch_size x vocab_size)
    model.get_logprobs_batched.return_value = torch.randn(3, 32000)
    
    return model


@pytest.fixture
def sample_prompts():
    """Sample prompt lists for testing."""
    return {
        "good": ["Hello", "What is 2+2?", "Tell me a story"],
        "bad": ["How to hack", "Make explosives", "Harmful content"],
    }
```

**tests/unit/test_evaluator.py:** (CORRECTED - proper mocking)
```python
"""Unit tests for Evaluator class.

These tests use proper mocking to avoid loading real models.
The key insight is that Evaluator.__init__ calls several methods
that need to be patched BEFORE instantiation.
"""
import pytest
from unittest.mock import Mock, MagicMock, patch
import torch


class TestRefusalDetection:
    """Test refusal marker detection (no model needed)."""
    
    @pytest.fixture
    def evaluator_with_mocked_init(self, mock_settings, mock_model):
        """Create Evaluator with mocked initialization.
        
        Patches load_prompts and model methods to avoid real inference.
        """
        with patch('heretic.evaluator.load_prompts') as mock_load:
            # Return minimal prompt lists
            mock_load.return_value = ["test prompt"]
            
            # Patch the base_logprobs computation
            with patch.object(mock_model, 'get_logprobs_batched') as mock_logprobs:
                mock_logprobs.return_value = torch.randn(1, 32000)
                
                # Patch count_refusals to avoid inference during init
                with patch('heretic.evaluator.Evaluator.count_refusals') as mock_count:
                    mock_count.return_value = 1
                    
                    from heretic.evaluator import Evaluator
                    evaluator = Evaluator(mock_settings, mock_model)
                    
        return evaluator
    
    def test_refusal_detection_basic(self, evaluator_with_mocked_init):
        """Test basic refusal marker detection."""
        evaluator = evaluator_with_mocked_init
        
        assert evaluator.is_refusal("I'm sorry, I can't help with that")
        assert evaluator.is_refusal("I cannot assist with harmful requests")
        assert not evaluator.is_refusal("Sure, I can help with that")
        assert not evaluator.is_refusal("Here's how to do it")
    
    def test_refusal_detection_case_insensitive(self, evaluator_with_mocked_init):
        """Test refusal detection is case-insensitive."""
        evaluator = evaluator_with_mocked_init
        
        assert evaluator.is_refusal("I'M SORRY")
        assert evaluator.is_refusal("i'm sorry")
        assert evaluator.is_refusal("I'm SoRrY")
    
    def test_refusal_detection_with_emphasis(self, evaluator_with_mocked_init):
        """Test refusal detection strips markdown emphasis."""
        evaluator = evaluator_with_mocked_init
        
        # Should handle "I *cannot* help" -> "I cannot help"
        assert evaluator.is_refusal("I *cannot* help with that")
    
    def test_regex_is_precompiled(self, evaluator_with_mocked_init):
        """Verify regex pattern is pre-compiled for performance."""
        evaluator = evaluator_with_mocked_init
        
        # Pattern should be compiled (has 'pattern' attribute)
        assert hasattr(evaluator.refusal_pattern, 'pattern')
        assert evaluator.refusal_pattern.pattern is not None


@pytest.mark.slow
@pytest.mark.integration
class TestEvaluatorIntegration:
    """Integration tests with real (tiny) model.
    
    These tests load gpt2 (small, ~500MB) for real inference.
    Marked slow so they don't run in CI by default.
    """
    
    def test_refusal_detection_real_model(self):
        """Integration test with real model."""
        pytest.skip("Requires gpt2 download - run with -m slow")
        # Implementation would load gpt2 and test real inference
```

**tests/unit/test_config.py:** (CORRECTED)
```python
"""Unit tests for Settings/Config."""
import os
import pytest
from pydantic import ValidationError


def test_settings_defaults():
    """Test default settings values."""
    from heretic.config import Settings
    
    settings = Settings(model="test-model")
    
    assert settings.model == "test-model"
    assert settings.n_trials == 200
    assert settings.batch_size == 0  # Auto-detect
    assert settings.dtypes == ["auto", "float16", "float32"]


def test_settings_validation_requires_model():
    """Test Pydantic validation catches missing model."""
    from heretic.config import Settings
    
    with pytest.raises(ValidationError):
        Settings()  # Missing required 'model' field


def test_settings_new_performance_options():
    """Test new performance optimization settings."""
    from heretic.config import Settings
    
    settings = Settings(
        model="test",
        compile=True,
        refusal_check_tokens=30,
    )
    
    assert settings.compile is True
    assert settings.refusal_check_tokens == 30


def test_settings_storage_and_study_name():
    """Test Optuna persistence settings."""
    from heretic.config import Settings
    
    settings = Settings(
        model="test",
        storage="sqlite:///test_study.db",
        study_name="my_study",
    )
    
    assert settings.storage == "sqlite:///test_study.db"
    assert settings.study_name == "my_study"
```

**Acceptance Criteria:**
- [ ] Pytest runs successfully with >70% coverage
- [ ] Unit tests use proper mocking with `patch()` (no model downloads, <5s execution)
- [ ] conftest.py placed at `tests/conftest.py` (NOT in subdirectory)
- [ ] Coverage config in `pyproject.toml` (NOT pytest.ini)
- [ ] Integration tests marked with `@pytest.mark.slow`
- [ ] Critical paths tested: refusal detection, config loading, new performance options
- [ ] CI workflow added to run unit tests on PRs
- [ ] Nightly CI runs integration tests

---

### Task 1.2: Add API Key Validation (Security Fix)
**Priority:** ðŸ”´ HIGH
**Effort:** 2-4 hours
**Risk:** MEDIUM (Command injection vulnerability)
**Dependencies:** Task 1.1 (for tests)

**Security Issue:**
`vast.py` passes API key to WSL via shell escaping. Current escaping handles single quotes but not all shell metacharacters. Risk: API key with backticks, dollar signs, or semicolons could execute arbitrary commands.

**Implementation:**
```python
# File: src/heretic/vast.py
# Location: VastConfig.from_env() method

import re

@classmethod
def from_env(cls) -> "VastConfig":
    """Load configuration from environment variables and .env file."""
    # ... existing code ...

    api_key = os.environ.get("VAST_API_KEY", "")

    # SECURITY: Validate API key format to prevent command injection
    if api_key:
        if not re.match(r'^[A-Za-z0-9_-]+$', api_key):
            raise ValueError(
                "VAST_API_KEY contains invalid characters. "
                "Only alphanumeric characters, hyphens, and underscores are allowed. "
                "This prevents command injection when using WSL."
            )

    return cls(
        api_key=api_key,
        local_models_dir=os.environ.get("LOCAL_MODELS_DIR", "./models"),
    )
```

**Test Cases:**
```python
# tests/unit/test_vast.py
import os
import pytest

def test_api_key_validation_valid():
    """Test valid API keys are accepted."""
    os.environ["VAST_API_KEY"] = "abc123-DEF_456"
    from heretic.vast import VastConfig
    config = VastConfig.from_env()  # Should pass
    del os.environ["VAST_API_KEY"]

def test_api_key_validation_rejects_semicolon():
    """Test semicolon injection is blocked."""
    os.environ["VAST_API_KEY"] = "key; rm -rf /"
    from heretic.vast import VastConfig
    with pytest.raises(ValueError, match="invalid characters"):
        VastConfig.from_env()
    del os.environ["VAST_API_KEY"]

def test_api_key_validation_rejects_backticks():
    """Test backtick injection is blocked."""
    os.environ["VAST_API_KEY"] = "key`whoami`"
    from heretic.vast import VastConfig
    with pytest.raises(ValueError, match="invalid characters"):
        VastConfig.from_env()
    del os.environ["VAST_API_KEY"]
```

**Acceptance Criteria:**
- [ ] API key validation prevents shell injection
- [ ] Clear error message for invalid keys
- [ ] Unit test covers validation logic
- [ ] Documentation updated in vast.py docstring

---

### Task 1.3: Document Web Search Behavior
**Priority:** ðŸŸ¡ MEDIUM
**Effort:** 2-3 hours
**Risk:** LOW (Documentation only)
**Dependencies:** None

**Note:** Verify that `chat_app.py` exists and contains a `WebSearcher` class before implementing. If not present, this task may need to be removed or modified.

**Files to Update:**

**1. chat_app.py - Add comprehensive docstring (if WebSearcher exists):**
```python
class WebSearcher:
    """Handles web searches using DuckDuckGo.

    Backend Strategy:
    ----------------
    - Primary: DuckDuckGo (reliable, no API key required)
    - Fallback: Currently none (future: Brave, Google)

    Region Configuration:
    --------------------
    - Default: 'wt-wt' (worldwide results)
    - Override: Set DDGS_REGION env var to country code
      Examples: 'us-en' (US English), 'uk-en' (UK English)

    Failure Modes:
    -------------
    - Backend unavailable: WebSearchError raised, model answers without search
    - No results: Returns empty list with "[No results]" context message
    - Rate limiting: Currently fails (future: exponential backoff retry)
    """
```

**2. README.md - Add Web Search section (if applicable):**
```markdown
## Web Search Feature

The chat interface supports automatic web search for questions requiring current information.

### Configuration

**Backend:** DuckDuckGo (no API key required)
**Region:** Worldwide by default (`wt-wt`)
**Max Results:** 5 per query

**Change Region:**
```bash
export DDGS_REGION=us-en  # US English results
```
```

**Acceptance Criteria:**
- [ ] WebSearcher class has comprehensive docstring (if exists)
- [ ] README.md documents configuration and usage (if applicable)
- [ ] Failure modes are clearly explained

---

### Task 1.4: Add OOM Recovery Strategy (NEW)
**Priority:** ðŸ”´ HIGH
**Effort:** 8-12 hours
**Risk:** HIGH (OOM crashes lose progress without proper handling)
**Dependencies:** None

**Objective:** Gracefully handle GPU out-of-memory errors during abliteration.

**Implementation:**

**1. Add OOM detection and recovery to model.py:**
```python
# File: src/heretic/model.py

import gc
from contextlib import contextmanager

@contextmanager
def handle_oom(retry_with_smaller_batch: bool = True):
    """Context manager for OOM-safe GPU operations.
    
    Usage:
        with handle_oom():
            model.get_logprobs_batched(prompts)
    """
    try:
        yield
    except torch.cuda.OutOfMemoryError as e:
        logger.warning(f"GPU OOM detected: {e}")
        
        # Clear GPU memory
        gc.collect()
        torch.cuda.empty_cache()
        
        if retry_with_smaller_batch:
            raise BatchSizeError(
                "Out of GPU memory. Try reducing batch_size or max_batch_size."
            ) from e
        else:
            raise

class BatchSizeError(Exception):
    """Raised when batch size is too large for available GPU memory."""
    pass
```

**2. Add memory monitoring utility:**
```python
# File: src/heretic/utils.py

def get_gpu_memory_info() -> dict:
    """Get current GPU memory usage.
    
    Returns:
        dict with keys: total_gb, used_gb, free_gb, utilization_pct
    """
    if not torch.cuda.is_available():
        return {"total_gb": 0, "used_gb": 0, "free_gb": 0, "utilization_pct": 0}
    
    total = torch.cuda.get_device_properties(0).total_memory
    reserved = torch.cuda.memory_reserved(0)
    allocated = torch.cuda.memory_allocated(0)
    free = total - reserved
    
    return {
        "total_gb": total / 1e9,
        "used_gb": allocated / 1e9,
        "free_gb": free / 1e9,
        "utilization_pct": (allocated / total) * 100,
    }
```

**3. Add graceful shutdown on repeated OOM:**
```python
# File: src/heretic/main.py (in optimization loop)

oom_count = 0
MAX_OOM_RETRIES = 3

try:
    score, kl_divergence, refusals = evaluator.get_score()
except BatchSizeError:
    oom_count += 1
    if oom_count >= MAX_OOM_RETRIES:
        print(f"[red]Repeated OOM errors ({oom_count}). Saving progress and exiting.[/]")
        # Progress is already saved via Optuna storage
        print(f"[yellow]Resume with: heretic {settings.model} --storage {settings.storage}[/]")
        return
    
    # Reduce batch size and retry
    settings.max_batch_size = max(1, settings.max_batch_size // 2)
    print(f"[yellow]Reducing max_batch_size to {settings.max_batch_size}[/]")
    continue
```

**Acceptance Criteria:**
- [ ] OOM errors are caught and logged
- [ ] GPU memory is cleared on OOM
- [ ] Batch size is reduced automatically
- [ ] Progress is preserved via Optuna storage
- [ ] Clear instructions for resuming after OOM
- [ ] Memory monitoring utility available

---

## Phase 2: Performance & Quality (Week 3-6)

### âœ… COMPLETED: In-Memory Weight Caching
**Status:** DONE (2026-01-29)
**Implementation:** `model.py:87-88`
**Speedup:** ~5-10x faster model reset per trial

### âœ… COMPLETED: torch.compile() Support
**Status:** DONE (2026-01-29)
**Implementation:** `config.py:70-73`, `model.py:90-92`
**Speedup:** ~1.5-2x inference (use `--compile` flag)

### âœ… COMPLETED: Early Stopping for Refusals
**Status:** DONE (2026-01-29)
**Implementation:** `config.py:85-88`, `evaluator.py:62-66`
**Speedup:** ~40-60% faster refusal counting (30 tokens vs 100)

### âœ… COMPLETED: Parallel KL + Refusal Evaluation
**Status:** DONE (2026-01-29)
**Implementation:** `evaluator.py:77-84`
**Speedup:** Concurrent execution via ThreadPoolExecutor

### âœ… COMPLETED: Optuna Persistence
**Status:** DONE (2026-01-29)
**Implementation:** `config.py:108-117`
**Feature:** `--storage` and `--study-name` flags for resume support

---

### Task 2.1: Benchmark Optuna Optimization Modes
**Priority:** ðŸŸ¡ MEDIUM
**Effort:** 20-28 hours (includes 8-12h GPU time + analysis)
**Expected Impact:** ~30% compute time reduction (with trade-offs)
**Dependencies:** Completed performance optimizations (must update baseline)

**âš ï¸ IMPORTANT CLARIFICATION:**

The plan originally claimed "~30% compute savings" from Optuna pruning. This requires clarification:

> **Pruning is NOT supported for multi-objective optimization in Optuna.**
> 
> To enable pruning, you must switch from multi-objective (Pareto front) to single-objective (weighted score).
> This sacrifices Pareto front diversity in exchange for faster convergence.

**Trade-off Analysis:**

| Mode | Pruning | Output | Use Case |
|------|---------|--------|----------|
| Multi-objective (current) | âŒ No | Pareto front (diverse solutions) | Research, exploration |
| Single-objective (new) | âœ… Yes | Single best weighted score | Production, speed |

**Benchmark Protocol:**

**âš ï¸ NOTE:** The baseline has changed significantly due to completed optimizations:
- In-memory weight caching: Trials now reset in ~1-2s instead of ~10s
- Early stopping: Refusal counting is ~40-60% faster
- Parallel evaluation: KL + refusals run concurrently

**Update benchmarks to reflect new baseline before comparing optimization modes.**

1. **Setup:**
   - Model: Qwen2.5-7B-Instruct (consistent baseline)
   - Hardware: A100 80GB (log GPU-hours for cost analysis)
   - Trials: 200 per mode
   - **Use latest code with all performance optimizations enabled**

2. **Run both modes:**
   ```bash
   # Multi-objective (current default)
   heretic Qwen/Qwen2.5-7B-Instruct --optimization-mode multi --n-trials 200 --compile

   # Single-objective (new, enables pruning)
   heretic Qwen/Qwen2.5-7B-Instruct --optimization-mode single --n-trials 200 --compile --kl-weight 0.5
   ```

3. **Metrics to track:**
   - Wall-clock time (minutes)
   - Trials completed vs pruned
   - Best trial quality (KL divergence + refusals)
   - Pareto front diversity (multi-objective only)

**Acceptance Criteria:**
- [ ] Benchmark results documented with new baseline (post-optimization)
- [ ] Clear explanation of multi-objective vs single-objective trade-off
- [ ] Recommendation: Keep multi-objective as default OR add `--fast` mode
- [ ] Update CLAUDE.md with optimization guidance

---

### Task 2.2: Add Model Weight Checksum Verification
**Priority:** ðŸŸ¡ MEDIUM
**Effort:** 8-12 hours
**Impact:** Detect corrupted/tampered models before loading
**Dependencies:** None

*(Implementation unchanged from original plan)*

**Acceptance Criteria:**
- [ ] Checksum generation script works
- [ ] Verification prevents loading tampered models
- [ ] Verification is optional (no checksums.json = no verification)
- [ ] Clear error messages on verification failure (with specific examples)
- [ ] Documentation in README.md

---

### Task 2.3: Implement Multi-GPU Parallel Trial Execution
**Priority:** ðŸŸ¡ MEDIUM
**Effort:** 60-80 hours (REVISED from 24-32h)
**Expected Impact:** 2-4x speedup on multi-GPU systems
**Dependencies:** Task 1.4 (OOM Recovery), PostgreSQL (external)

**âš ï¸ COMPLEXITY WARNING:**

This task is more complex than originally estimated due to:

1. **Memory Constraints:**
   - Each parallel trial needs a separate Model instance
   - 32B model = ~60GB per instance
   - 4Ã— RTX 4090 (96GB total VRAM) = **only 1-2 parallel trials**, not 4
   - Must implement memory-aware job scheduling

2. **Conflict with In-Memory Weight Caching:**
   - Current optimization caches `state_dict` per process
   - Multi-process = each process needs its own cache = N Ã— memory
   - May need to disable caching for parallel mode OR implement shared memory

3. **PostgreSQL Migration:**
   - SQLite does not support concurrent writes
   - Users must migrate existing studies before enabling parallel mode
   - Migration script and clear documentation required

**Revised Implementation Strategy:**

**Option A: Process-per-GPU (Recommended)**
```python
# Spawn separate processes, each assigned to one GPU
# Each process loads its own model instance
# Requires PostgreSQL for trial coordination

import os
from multiprocessing import Process

def run_trial_on_gpu(gpu_id: int, study_name: str, storage_url: str):
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    # Load model, run optimization
    ...

# Launch one process per GPU
for gpu_id in range(torch.cuda.device_count()):
    p = Process(target=run_trial_on_gpu, args=(gpu_id, study_name, storage_url))
    p.start()
```

**Option B: Memory-Aware Scheduling (Advanced)**
```python
# Calculate available memory per GPU
# Schedule trials based on memory requirements
# May allow 2 trials on 80GB GPU for smaller models

def calculate_max_parallel_trials(model_size_gb: float) -> int:
    """Calculate how many parallel trials fit in available GPU memory."""
    available_memory = get_total_gpu_memory_gb()
    safety_margin = 0.9  # Reserve 10% for overhead
    
    # Account for in-memory weight caching (2x model size)
    memory_per_trial = model_size_gb * 2 if use_weight_caching else model_size_gb
    
    return int((available_memory * safety_margin) / memory_per_trial)
```

**Migration Script:**
```python
# File: scripts/migrate_optuna_study.py
#!/usr/bin/env python3
"""Migrate Optuna study from SQLite to PostgreSQL.

Required before enabling parallel_trials=true.
"""
import optuna
import sys

def migrate_study(source_url: str, target_url: str, study_name: str):
    print(f"Migrating study '{study_name}' from SQLite to PostgreSQL...")
    
    source_storage = optuna.storages.get_storage(source_url)
    target_storage = optuna.storages.get_storage(target_url)
    
    optuna.copy_study(
        from_study_name=study_name,
        from_storage=source_storage,
        to_storage=target_storage,
        to_study_name=study_name,
    )
    
    print(f"Migration complete!")

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("Usage: migrate_optuna_study.py <source_url> <target_url> <study_name>")
        print("\nExample:")
        print("  python migrate_optuna_study.py \\")
        print("    sqlite:///heretic_study.db \\")
        print("    postgresql://heretic:heretic@localhost/heretic_study \\")
        print("    heretic_study")
        sys.exit(1)
    
    migrate_study(sys.argv[1], sys.argv[2], sys.argv[3])
```

**Acceptance Criteria:**
- [ ] Parallel mode works on multi-GPU systems
- [ ] Memory-aware scheduling prevents OOM
- [ ] PostgreSQL requirement validated at startup with clear error message
- [ ] Migration script and documentation provided
- [ ] Performance improvement measured and documented (realistic expectations)
- [ ] Handles interaction with in-memory weight caching

---

### Task 2.4: Checkpoint/Resume Robustness (NEW)
**Priority:** ðŸŸ¡ MEDIUM
**Effort:** 4-8 hours
**Impact:** Reliable experiment resumption
**Dependencies:** Pairs with Task 2.3

**Objective:** Ensure abliteration can reliably resume after interruption.

**Implementation:**

**1. SQLite corruption handling:**
```python
# File: src/heretic/main.py

def load_or_create_study(storage_url: str, study_name: str):
    """Load existing study or create new one with corruption handling."""
    try:
        study = optuna.load_study(
            study_name=study_name,
            storage=storage_url,
        )
        print(f"Resuming study '{study_name}' with {len(study.trials)} existing trials")
        return study
    except optuna.exceptions.DuplicatedStudyError:
        # Study exists but may be corrupted
        print(f"[yellow]Study '{study_name}' exists. Loading...[/]")
        return optuna.load_study(study_name=study_name, storage=storage_url)
    except Exception as e:
        if "database is locked" in str(e).lower():
            print(f"[red]Database is locked. Another process may be using it.[/]")
            print(f"[yellow]If not, delete the .db file and restart.[/]")
            raise
        elif "disk" in str(e).lower() or "corrupt" in str(e).lower():
            print(f"[red]Database may be corrupted: {e}[/]")
            print(f"[yellow]Backup and delete the .db file to start fresh.[/]")
            raise
        raise
```

**2. Study name conflict resolution:**
```python
def generate_unique_study_name(base_name: str, storage_url: str) -> str:
    """Generate unique study name if base name exists."""
    from datetime import datetime
    
    try:
        optuna.load_study(study_name=base_name, storage=storage_url)
        # Study exists - append timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{base_name}_{timestamp}"
    except:
        return base_name
```

**3. Crash recovery verification test:**
```python
# tests/integration/test_resume.py

@pytest.mark.slow
def test_resume_after_interruption():
    """Test that abliteration can resume after simulated crash."""
    storage = "sqlite:///test_resume.db"
    study_name = "test_resume"
    
    # Run 5 trials, then "crash"
    # ... run heretic with n_trials=5 ...
    
    # Resume and verify
    # ... run heretic with same storage/study_name ...
    # ... verify trials continue from 6 ...
    
    # Cleanup
    os.remove("test_resume.db")
```

**Acceptance Criteria:**
- [ ] SQLite corruption detected and reported clearly
- [ ] Database lock errors have actionable messages
- [ ] Study name conflicts handled gracefully
- [ ] Integration test verifies resume works

---

### Task 2.5: Set Up CI/CD Pipeline (MOVED from Phase 3)
**Priority:** ðŸŸ¡ MEDIUM
**Effort:** 12-20 hours
**Impact:** Automation and quality gates
**Dependencies:** Task 1.1 (tests must exist)

**Rationale for moving to Phase 2:**
CI/CD enables safer iteration on performance work and catches regressions early.

**Implementation:**

**1. Test Workflow (.github/workflows/test.yml):**
```yaml
name: Test

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --all-extras --dev

      - name: Run unit tests
        run: uv run pytest -m "not slow" --cov=src/heretic --cov-report=xml

      - name: Check coverage threshold
        run: uv run coverage report --fail-under=70
```

**2. Lint Workflow (.github/workflows/lint.yml):**
```yaml
name: Lint

on: [push, pull_request]

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v1

      - name: Install dependencies
        run: uv sync --dev

      - name: Check formatting
        run: uv run ruff format --check .

      - name: Run linter
        run: uv run ruff check .
```

**3. Pre-commit Hooks (.pre-commit-config.yaml):**
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.8.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
        args: ['--maxkb=5000']

  # NOTE: Removed pytest from pre-commit - too slow for every commit
  # Run tests manually or in CI instead
```

**Acceptance Criteria:**
- [ ] CI runs tests on all PRs
- [ ] Coverage threshold enforced (>70%)
- [ ] Pre-commit hooks enforce formatting (no pytest - too slow)
- [ ] Quality gates prevent bad merges
- [ ] README badges show build status

---

## Phase 3: Long-term Improvements (Week 7-12)

### Task 3.1: Design Plugin Architecture for Direction Extractors
**Priority:** ðŸŸ¢ LOW
**Effort:** 24-36 hours (revised from 20-30h for backward compat)
**Impact:** Enables community contributions and experimentation
**Dependencies:** None

**Note:** When implementing, do NOT use `Settings._default_settings()` - this method doesn't exist. Instead:
```python
# WRONG:
settings = Settings._default_settings()

# CORRECT:
from heretic.config import Settings
settings = Settings(model="placeholder")  # Create with defaults
```

*(Rest of implementation unchanged from original plan)*

**Acceptance Criteria:**
- [ ] DirectionExtractor base class implemented
- [ ] RefusalDirectionExtractor works (backward compatible)
- [ ] VerbosityDirectionExtractor migrated
- [ ] CLI --direction flag functional
- [ ] Documentation in CONTRIBUTING.md
- [ ] Existing experiments still work

---

### Task 3.2: Add Structured Logging with Metrics Export
**Priority:** ðŸŸ¢ LOW
**Effort:** 16-24 hours
**Impact:** Better observability for long experiments
**Dependencies:** None

*(Implementation unchanged from original plan)*

**Note:** structlog is preferred over loguru for production environments requiring structured logging and complex processing (per 2024-2025 best practices).

**Acceptance Criteria:**
- [ ] structlog configured and working
- [ ] main.py uses structured logging
- [ ] JSON logs written to heretic.log
- [ ] Optional Prometheus metrics functional
- [ ] Documentation updated

---

## Timeline Summary

| Phase | Week | Tasks | Effort (Revised) | Status |
|-------|------|-------|------------------|--------|
| Phase 1 | 1-2 | 1.1-1.4 (Testing, Security, Docs, OOM) | 44-59h | ðŸ”´ TODO |
| Phase 2 | 3-6 | 2.1-2.5 (Benchmarks, Checksums, Multi-GPU, CI/CD) | 104-148h | ðŸŸ¡ PARTIAL (perf optimizations done) |
| Phase 3 | 7-12 | 3.1-3.2 (Plugins, Logging) | 40-60h | ðŸ”´ TODO |
| **Total** | **12 weeks** | **11 tasks** | **188-267h** | **In Progress** |

**Completed Work (not in original estimates):**
- âœ… In-memory weight caching (~4h)
- âœ… torch.compile() support (~2h)
- âœ… Early stopping for refusals (~3h)
- âœ… Parallel KL + refusal evaluation (~2h)
- âœ… Optuna persistence (~2h)

---

## Success Metrics

### Quality Improvements
- **Before:** Quality score 8.2/10, Testing 2/10
- **After Phase 1:** Quality score 9.0/10, Testing 8/10
- **After Phase 3:** Quality score 9.5/10, Testing 9/10

### Performance Gains (UPDATED)
- **Completed:**
  - In-memory weight caching: ~5-10x faster model reset
  - torch.compile(): ~1.5-2x inference speedup
  - Early stopping: ~40-60% faster refusal counting
  - Parallel evaluation: Concurrent KL + refusals

- **Remaining:**
  - Multi-GPU: 2-4x speedup (complex, 60-80h)
  - Optuna pruning: ~30% (requires single-objective mode)

### Security Enhancements
- API key validation: Command injection prevented
- Checksum verification: Tampered models detected

---

## Risk Management

### High Risk Items
1. **Testing Implementation** - May uncover existing bugs
   - Mitigation: Fix bugs as discovered, document workarounds

2. **Multi-GPU Complexity** - Memory constraints, database concurrency
   - Mitigation: Realistic expectations, thorough testing, PostgreSQL docs

3. **Breaking Changes** - Plugin architecture may break experiments
   - Mitigation: Backward compatibility layer, migration guide

### Dependencies
- PostgreSQL required for multi-GPU (add to docs)
- pytest required for testing (add to dev dependencies)
- Docker optional but recommended (provide compose file)

---

## Rollback Procedures

In case any phase encounters critical issues, use these rollback strategies:

### Phase 1 Rollbacks

**Task 1.1 - Testing Issues:**
```bash
# If tests reveal critical bugs that block development:
# 1. Disable failing tests temporarily
pytest -m "not slow and not broken"

# 2. Mark specific tests as xfail (expected to fail)
@pytest.mark.xfail(reason="Known issue #123, fix in progress")
def test_problematic_feature():
    pass

# 3. Skip tests for specific modules
pytest --ignore=tests/unit/test_broken_module.py
```

**Task 1.2 - API Validation Breaks Legitimate Keys:**
```python
# Add environment variable bypass (emergency only)
if os.environ.get("SKIP_API_VALIDATION") == "true":
    logger.warning("API key validation DISABLED via env var")
else:
    # ... normal validation
```

### Phase 2 Rollbacks

**Task 2.1 - Benchmark Inconclusive:**
- Keep multi-objective as default
- Document findings in knowledge.md (even if negative result)
- No code changes needed to rollback

**Task 2.2 - Checksum Performance Issues:**
```python
# Disable verification via environment variable
if os.environ.get("SKIP_CHECKSUM_VERIFICATION") == "true":
    logger.warning("Checksum verification DISABLED")
    return True, "Skipped via env var"
```

**Task 2.3 - Multi-GPU Bugs:**
```toml
# Disable in config.toml
parallel_trials = false  # Rollback to single-GPU
```

**Rollback PostgreSQL â†’ SQLite:**
```bash
python scripts/migrate_optuna_study.py \
  postgresql://localhost/heretic_study \
  sqlite:///heretic_study.db \
  heretic_study
```

### Phase 3 Rollbacks

**Task 3.1 - Plugin Architecture Breaks Experiments:**
```python
# Keep legacy dataset loading alongside plugin system
if settings.direction in ["refusal", "verbosity"]:
    extractor = get_extractor(settings.direction)
else:
    # Fallback to legacy hardcoded loading
    good_prompts = load_prompts(settings.good_prompts)
    bad_prompts = load_prompts(settings.bad_prompts)
```

**Task 3.2 - Structured Logging Issues:**
```python
# Keep both logging systems during transition
USE_STRUCTURED_LOGGING = os.environ.get("STRUCTURED_LOGGING", "false") == "true"

if USE_STRUCTURED_LOGGING:
    logger.info("trial_completed", trial_id=trial_index, kl=kl_div)
else:
    print(f"Trial {trial_index} completed: KL={kl_div}")
```

### Emergency Full Rollback

If multiple tasks fail and the codebase becomes unstable:

```bash
# 1. Revert to last stable commit
git log --oneline  # Find last stable commit
git revert --no-commit <commit>..HEAD
git commit -m "Emergency rollback: reverting implementation plan changes"

# 2. Remove test infrastructure
rm -rf tests/
git checkout pyproject.toml  # Restore original dependencies

# 3. Disable new features
export SKIP_API_VALIDATION=true
export SKIP_CHECKSUM_VERIFICATION=true
parallel_trials = false  # In config.toml
```

---

## Review Checklist

- [x] Phase 1 priorities align with business needs
- [x] Timeline is realistic (12 weeks for 11 tasks)
- [x] Acceptance criteria are measurable
- [x] Risk mitigation strategies are adequate
- [x] Documentation updates are included in each task
- [x] Performance benchmarks will be tracked
- [x] **NEW:** Completed optimizations are documented
- [x] **NEW:** Test fixtures use proper mocking
- [x] **NEW:** Effort estimates are realistic
- [x] **NEW:** Optuna pruning trade-offs are explained

---

## Changelog

### 2026-01-29: Post-Optimization Review
- **UPDATED:** Executive Summary to reflect completed performance optimizations
- **ADDED:** Task Dependency Graph
- **ADDED:** Definition of Done section
- **ADDED:** Task 1.4 (OOM Recovery Strategy)
- **ADDED:** Task 2.4 (Checkpoint/Resume Robustness)
- **MOVED:** Task 2.5 (CI/CD) from Phase 3 to Phase 2
- **FIXED:** Task 1.1 test fixtures (proper mocking, conftest.py location, coverage config in pyproject.toml)
- **FIXED:** Task 2.3 effort estimate (60-80h instead of 24-32h)
- **FIXED:** Optuna pruning claim (clarified multi-objective limitation)
- **FIXED:** Plugin architecture example (Settings._default_settings() doesn't exist)
- **REMOVED:** Pre-commit pytest hook (too slow)
- **MARKED:** Phase 2 performance optimizations as COMPLETE

---

## Next Steps

1. **Start Phase 1** - Begin with Task 1.1 (pytest test suite with corrected fixtures)
2. **Track progress** - Update task status weekly
3. **Iterate** - Adjust plan based on findings

**Questions? Concerns? Ready to start?**
